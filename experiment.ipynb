{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Fixed Distance Estimation for Campus Navigation Assistant\n",
        "\n",
        "This notebook implements a robust distance estimation module for the Mobile-Based Campus Navigation Assistant, optimized for iPhone 14 images. It uses the pretrained ResNet50 model (`resnet50_multiclass_building_detection_full.pth`) for landmark detection and estimates distances using object size comparison and triangulation. YOLOv5 detects doors (reference object, height: 2 meters) with a manual bounding box fallback. Stereo rectification enhances triangulation, and interactive widgets simplify input. The model loading is fixed for PyTorch 2.6+ to handle full model serialization.\n",
        "\n",
        "## Objectives\n",
        "- Load ResNet50 model, handling full model serialization in PyTorch 2.6+.\n",
        "- Detect doors using YOLOv5 with manual input fallback.\n",
        "- Estimate distance using:\n",
        "  - Object size comparison (iPhone 14 specs).\n",
        "  - Triangulation with stereo rectification.\n",
        "- Provide interactive image upload and baseline input.\n",
        "- Save results in JSON for Phase 4 integration.\n",
        "\n",
        "## iPhone 14 Camera Specs\n",
        "- Focal length: 4.25mm.\n",
        "- Sensor size: 7.6mm x 5.7mm.\n",
        "- Resolution: 4032x3024 pixels.\n",
        "- Focal length in pixels: ~2253 pixels.\n",
        "\n",
        "## Input Requirements\n",
        "- Two images in `images/` folder (e.g., `image1.jpg`, `image2.jpg`).\n",
        "- Baseline distance (e.g., 0.5m) between camera positions.\n",
        "- `resnet50_multiclass_building_detection_full.pth` and `annotations.csv` in working directory.\n",
        "\n",
        "## Outputs\n",
        "- Estimated distances (size comparison and triangulation).\n",
        "- Visualizations (annotated image, triangulation geometry).\n",
        "- Saved files: `result_annotated.jpg`, `distance_results.json`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from ultralytics import YOLO\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Annotations and Define Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_annotations():\n",
        "    if not os.path.exists('annotations.csv'):\n",
        "        raise FileNotFoundError('annotations.csv not found')\n",
        "    annotations = pd.read_csv('annotations.csv')\n",
        "    class_names = sorted(annotations['label'].unique())\n",
        "    class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "    idx_to_class = {idx: cls for cls, idx in class_to_idx.items()}\n",
        "    return class_names, class_to_idx, idx_to_class\n",
        "\n",
        "class_names, class_to_idx, idx_to_class = load_annotations()\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Classes: {class_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Pretrained ResNet50 Model\n",
        "\n",
        "Fixed to load full model directly with `weights_only=False`, with fallback for state_dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_resnet_model():\n",
        "    model_file = 'resnet50_multiclass_building_detection_full.pth'\n",
        "    if not os.path.exists(model_file):\n",
        "        raise FileNotFoundError(f'{model_file} not found')\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    try:\n",
        "        # Try loading as full model\n",
        "        model = torch.load(model_file, map_location=device, weights_only=False)\n",
        "        # Verify the model has the correct number of classes\n",
        "        if model.fc.out_features != num_classes:\n",
        "            raise ValueError(f'Model has {model.fc.out_features} output classes, expected {num_classes}')\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading full model: {str(e)}\")\n",
        "        print(\"Attempting to load as state_dict...\")\n",
        "        # Fallback to state_dict loading\n",
        "        model = resnet50(weights=None)\n",
        "        model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "        try:\n",
        "            state_dict = torch.load(model_file, map_location=device, weights_only=True)\n",
        "            model.load_state_dict(state_dict)\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(f'Failed to load model as state_dict: {str(e2)}')\n",
        "    \n",
        "    model.eval()\n",
        "    return model.to(device), device\n",
        "\n",
        "model, device = load_resnet_model()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load YOLOv5 for Door Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yolo_model = YOLO('yolov5s.pt')\n",
        "yolo_conf_threshold = 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Landmark Detection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_landmark(image_path):\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f'Image {image_path} not found')\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    try:\n",
        "        exif = image._getexif()\n",
        "        if exif:\n",
        "            from PIL.ExifTags import TAGS\n",
        "            for tag, value in exif.items():\n",
        "                if TAGS.get(tag) == 'Orientation':\n",
        "                    if value == 3:\n",
        "                        image = image.rotate(180, expand=True)\n",
        "                    elif value == 6:\n",
        "                        image = image.rotate(270, expand=True)\n",
        "                    elif value == 8:\n",
        "                        image = image.rotate(90, expand=True)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        confidence, predicted = torch.max(probs, 1)\n",
        "        predicted_class = idx_to_class[predicted.item()]\n",
        "    return predicted_class, confidence.item(), image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Door Detection with YOLOv5 and Fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_door(image_path, manual_bbox=None):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f'Failed to load image {image_path}')\n",
        "    scale_factor = 0.5\n",
        "    img_resized = cv2.resize(img, None, fx=scale_factor, fy=scale_factor)\n",
        "    \n",
        "    if manual_bbox:\n",
        "        x1, y1, x2, y2 = manual_bbox\n",
        "        pixel_height = (y2 - y1) / scale_factor\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(img, 'Manual Door', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        return pixel_height, img, 0.5\n",
        "    \n",
        "    results = yolo_model(img_resized, conf=yolo_conf_threshold)\n",
        "    detections = results.pandas().xyxy[0]\n",
        "    door_detections = detections[detections['name'] == 'door']\n",
        "    \n",
        "    if door_detections.empty:\n",
        "        print('No door detected. Use manual bounding box or try another image.')\n",
        "        return None, img, 0.0\n",
        "    \n",
        "    door = door_detections.loc[door_detections['confidence'].idxmax()]\n",
        "    x1, y1, x2, y2 = [int(v / scale_factor) for v in [door['xmin'], door['ymin'], door['xmax'], door['ymax']]]\n",
        "    pixel_height = y2 - y1\n",
        "    confidence = door['confidence']\n",
        "    \n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.putText(img, f'Door (Conf: {confidence:.2f})', (x1, y1-10), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "    \n",
        "    return pixel_height, img, confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Distance Estimation via Object Size Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_distance_size_comparison(pixel_height, real_height=2.0, focal_length=2253):\n",
        "    if pixel_height is None or pixel_height <= 0:\n",
        "        return None, 0.0\n",
        "    try:\n",
        "        distance = (focal_length * real_height) / pixel_height\n",
        "        if distance <= 0 or distance > 100:\n",
        "            return None, 0.0\n",
        "        confidence = min(1.0, pixel_height / 500)\n",
        "        return distance, confidence\n",
        "    except:\n",
        "        return None, 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Distance Estimation via Triangulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_distance_triangulation(image_path1, image_path2, baseline=0.5, focal_length=2253):\n",
        "    img1 = cv2.imread(image_path1, cv2.IMREAD_GRAYSCALE)\n",
        "    img2 = cv2.imread(image_path2, cv2.IMREAD_GRAYSCALE)\n",
        "    if img1 is None or img2 is None:\n",
        "        raise ValueError('Failed to load one or both images')\n",
        "    \n",
        "    scale_factor = 0.5\n",
        "    img1 = cv2.resize(img1, None, fx=scale_factor, fy=scale_factor)\n",
        "    img2 = cv2.resize(img2, None, fx=scale_factor, fy=scale_factor)\n",
        "    \n",
        "    orb = cv2.ORB_create()\n",
        "    kp1, des1 = orb.detectAndCompute(img1, None)\n",
        "    kp2, des2 = orb.detectAndCompute(img2, None)\n",
        "    if des1 is None or des2 is None:\n",
        "        print('Insufficient keypoints detected')\n",
        "        return None, 0.0\n",
        "    \n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    matches = bf.match(des1, des2)\n",
        "    matches = sorted(matches, key=lambda x: x.distance)\n",
        "    if len(matches) < 20:\n",
        "        print('Insufficient matches for triangulation')\n",
        "        return None, 0.0\n",
        "    \n",
        "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
        "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
        "    \n",
        "    F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)\n",
        "    if F is None:\n",
        "        return None, 0.0\n",
        "    inliers = mask.ravel() == 1\n",
        "    pts1 = pts1[inliers]\n",
        "    pts2 = pts2[inliers]\n",
        "    if len(pts1) < 10:\n",
        "        print('Insufficient inlier matches')\n",
        "        return None, 0.0\n",
        "    \n",
        "    h, w = img1.shape\n",
        "    _, H1, H2 = cv2.stereoRectifyUncalibrated(pts1, pts2, F, (w, h))\n",
        "    img1_rect = cv2.warpPerspective(img1, H1, (w, h))\n",
        "    img2_rect = cv2.warpPerspective(img2, H2, (w, h))\n",
        "    \n",
        "    stereo = cv2.StereoBM_create(numDisparities=64, blockSize=15)\n",
        "    disparity = stereo.compute(img1_rect, img2_rect)\n",
        "    valid_disparities = disparity[disparity > 0] / 16.0\n",
        "    if len(valid_disparities) == 0:\n",
        "        return None, 0.0\n",
        "    disparity_median = np.median(valid_disparities) * scale_factor\n",
        "    \n",
        "    try:\n",
        "        distance = (baseline * focal_length) / disparity_median\n",
        "        if distance <= 0 or distance > 100:\n",
        "            return None, 0.0\n",
        "        confidence = min(1.0, len(valid_disparities) / 1000)\n",
        "        return distance, confidence\n",
        "    except:\n",
        "        return None, 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Triangulation Geometry Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_triangulation_geometry(baseline, distance, landmark):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot([0], [0], 'ro', label='Camera 1')\n",
        "    plt.plot([baseline], [0], 'bo', label='Camera 2')\n",
        "    plt.plot([baseline/2], [distance], 'g^', label=f'{landmark}')\n",
        "    plt.plot([0, baseline/2], [0, distance], 'r--')\n",
        "    plt.plot([baseline, baseline/2], [0, distance], 'b--')\n",
        "    plt.xlabel('X (meters)')\n",
        "    plt.ylabel('Distance (meters)')\n",
        "    plt.title('Triangulation Geometry')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.axis('equal')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Visualization and Result Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_and_save_results(image_path1, image_path2, landmark, landmark_conf, \n",
        "                              distance_size, size_conf, distance_triang, triang_conf, \n",
        "                              img_with_door, baseline):\n",
        "    img_with_door_rgb = cv2.cvtColor(img_with_door, cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(img_with_door_rgb)\n",
        "    plt.title(f\"Landmark: {landmark} (Conf: {landmark_conf:.2f})\\n\"\n",
        "              f\"Distance (Size): {distance_size:.2f if distance_size else 'N/A'}m (Conf: {size_conf:.2f})\\n\"\n",
        "              f\"Distance (Triang): {distance_triang:.2f if distance_triang else 'N/A'}m (Conf: {triang_conf:.2f})\", fontsize=12)\n",
        "    plt.axis('off')\n",
        "    plt.savefig('result_annotated.jpg', bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    if distance_triang is not None:\n",
        "        visualize_triangulation_geometry(baseline, distance_triang, landmark)\n",
        "    \n",
        "    results = {\n",
        "        'landmark': landmark,\n",
        "        'landmark_confidence': landmark_conf,\n",
        "        'distance_size': distance_size,\n",
        "        'size_confidence': size_conf,\n",
        "        'distance_triangulation': distance_triang,\n",
        "        'triangulation_confidence': triang_conf,\n",
        "        'image1_path': image_path1,\n",
        "        'image2_path': image_path2,\n",
        "        'baseline': baseline\n",
        "    }\n",
        "    with open('distance_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Interactive Input Widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image1_upload = widgets.FileUpload(accept='.jpg,.png', description='Image 1')\n",
        "image2_upload = widgets.FileUpload(accept='.jpg,.png', description='Image 2')\n",
        "baseline_input = widgets.FloatText(value=0.5, description='Baseline (m):')\n",
        "manual_bbox_check = widgets.Checkbox(value=False, description='Manual Door BBox')\n",
        "bbox_x1 = widgets.IntText(value=100, description='x1:')\n",
        "bbox_y1 = widgets.IntText(value=100, description='y1:')\n",
        "bbox_x2 = widgets.IntText(value=200, description='x2:')\n",
        "bbox_y2 = widgets.IntText(value=300, description='y2:')\n",
        "run_button = widgets.Button(description='Run Estimation')\n",
        "output = widgets.Output()\n",
        "\n",
        "def save_uploaded_image(upload_widget, filename):\n",
        "    if upload_widget.value:\n",
        "        uploaded_file = list(upload_widget.value.values())[0]\n",
        "        os.makedirs('images', exist_ok=True)\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(uploaded_file['content'])\n",
        "        return filename\n",
        "    return None\n",
        "\n",
        "def on_run_button_clicked(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        try:\n",
        "            image_path1 = save_uploaded_image(image1_upload, 'images/image1.jpg')\n",
        "            image_path2 = save_uploaded_image(image2_upload, 'images/image2.jpg')\n",
        "            if not image_path1 or not image_path2:\n",
        "                print('Please upload both images')\n",
        "                return\n",
        "            \n",
        "            baseline = baseline_input.value\n",
        "            if baseline <= 0:\n",
        "                print('Baseline must be positive')\n",
        "                return\n",
        "            \n",
        "            manual_bbox = None\n",
        "            if manual_bbox_check.value:\n",
        "                manual_bbox = [bbox_x1.value, bbox_y1.value, bbox_x2.value, bbox_y2.value]\n",
        "            \n",
        "            landmark, landmark_conf, pil_image = detect_landmark(image_path1)\n",
        "            pixel_height, img_with_door, door_conf = detect_door(image_path1, manual_bbox)\n",
        "            distance_size, size_conf = estimate_distance_size_comparison(pixel_height)\n",
        "            distance_triang, triang_conf = estimate_distance_triangulation(image_path1, image_path2, baseline)\n",
        "            \n",
        "            visualize_and_save_results(image_path1, image_path2, landmark, landmark_conf, \n",
        "                                     distance_size, size_conf, distance_triang, triang_conf, \n",
        "                                     img_with_door, baseline)\n",
        "            \n",
        "            print(f\"Detected Landmark: {landmark} (Confidence: {landmark_conf:.2f})\")\n",
        "            print(f\"Distance (Size): {distance_size if distance_size else 'N/A'} meters (Confidence: {size_conf:.2f})\")\n",
        "            print(f\"Distance (Triang): {distance_triang if distance_triang else 'N/A'} meters (Confidence: {triang_conf:.2f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\")\n",
        "\n",
        "run_button.on_click(on_run_button_clicked)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    image1_upload, image2_upload, baseline_input,\n",
        "    manual_bbox_check, bbox_x1, bbox_y1, bbox_x2, bbox_y2,\n",
        "    run_button, output\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- **Model Loading Fix**: Loads the full model directly with `weights_only=False`, with a fallback to `state_dict` loading.\n",
        "- **PyTorch 2.6+**: Uses `weights=None` to replace deprecated `pretrained`.\n",
        "- **iPhone 14 Optimization**: Focal length (2253 pixels) and EXIF handling.\n",
        "- **Door Detection**: YOLOv5 with manual bbox fallback.\n",
        "- **Triangulation**: Stereo rectification and fundamental matrix filtering.\n",
        "- **Outputs**: JSON file (`distance_results.json`) for Phase 4.\n",
        "- **Alternative**: If you prefer `weights_only=True`, re-save the model as a `state_dict` (see below)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
